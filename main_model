import pandas as pd
import numpy as np
import shap
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.neural_network import MLPRegressor
from sklearn.feature_selection import VarianceThreshold
import os

#AS OF 02/28/2025, THIS IS THE MOST EFFECTIVE MODEL
#version update: 03/10/2025

# File paths
input_file = r"" #input your directory here
test_file = r"" #input your directory here

print("Loading training data...")
# Load the training data
RegData = pd.read_excel(input_file)

print("Loading testing data...")
# Load the testing data
test_data = pd.read_excel(test_file)

# Choose dependent variable (change as needed)
dep_var = "A" #place one of A, B, C, D, EU, L, or I

# Check if dependent variable exists in both training and test data
if dep_var in RegData.columns and dep_var in test_data.columns:
    print(f"Dependent variable '{dep_var}' found in both datasets.")

    # Select independent variables depending on the variable that you want to predict
    indep_vars = [
        ##Trait A Words

        # "AMBITIOUS", "ANALYTICAL", "ASTUTE", "BOLD", "CLEVER", "COMMANDING", "COURAGEOUS", "CREATIVE", "DARING",
        # "DECISIVE", "DEDUCTIVE", "DEMANDING", "DETERMINED", "DOGMATIC", "DYNAMIC", "EXPERIMENTAL", "FARSIGHTED",
        # "FEARLESS", "INCISIVE", "INDIVIDUALISTIC", "INFLUENTIAL", "INNOVATIVE", "INQUISITIVE", "INSIGHTFUL",
        # "INTELLIGENT", "INVENTIVE", "KEEN", "KNOWLEDGEABLE", "OPPORTUNISTIC", "ORIGINAL", "PERCEPTIVE", "PERSISTENT",
        # "PURPOSEFUL", "RELENTLESS", "RESOLUTE", "RESOURCEFUL", "SELF-ASSURED", "SELF-CENTERED", "SELF-CONFIDENT",
        # "SELF-RELIANT", "SHREWD", "STRONG-WILLED", "THRIFTY", "UNWAVERING", "VENTURESOME", "ACUTE", "IDEALISTIC",

        ##Trait B Words
        # "APPROACHABLE", "ATTRACTIVE", "CHATTY", "CHEERFUL", "COMMUNICATIVE", "CONVERSATIONAL",
        # "EASILY-EXCITED", "ENDEARING", "ENTERTAINING", "EXPRESSIVE", "FACILITATING", "FASHIONABLE", "GOOD-MIXER",
        # "GULLIBLE", "IMMATURE", "IMPULSIVE", "INDULGENT", "INTERACTIVE", "JOVIAL", "LIFE-OF-THE-PARTY", "LIKABLE",
        # "MOODY", "OUTGOING", "PLAYFUL", "POPULAR", "SENSITIVE", "SOCIABLE",  "SPONTANEOUS",
        # "STATUS-CONSCIOUS", "SUGGESTIBLE", "SUPPORTIVE", "TALKATIVE", "TEMPERAMENTAL", "APPEALING", "CRISP",
        # "EMOTIONAL", "FLUENT", "LOQUACIOUS", "RESPECTFUL", "SENTIMENTAL", "SPATIAL", "RELAXED",
        # "COMFORTABLE", "POLISHED", "EASY-GOING", "RESOURCEFUL",

        ##Trait C Words
        # "ACCEPTING", "AT-EASE", "CALM", "COMFORTABLE", "EASY-GOING", "EFFICIENT", "EVEN-TEMPERED", "LAID-BACK",
        # "LINGERING", "NONCHALANT", "PATIENT", "PEACEFUL", "PLACID", "PONDERING", "PREDICTABLE", "RELAXED", "RESTFUL",
        # "STEADY", "TRANQUIL", "UNCONCERNED", "UNHURRIED", "UNRUFFLED", "RESTLESS", "SINGLE-MINDED", "FLUENT",
        # "THOROUGH","PAINSTAKING", "PRECISE", "FASTIDIOUS", "NIT-PICKY", "SYSTEMATIC",

        ##Trait D Words
        # "ANALYTICAL", "ASTUTE", "DEDUCTIVE", "FARSIGHTED", "INCISIVE", "INSIGHTFUL", "INTELLIGENT", "KEEN",
        # "KNOWLEDGEABLE", "PERCEPTIVE", "SHREWD",  "ACUTE", "ACCURATE", "AWARE", "CAUTIOUS", "COMPLIANT",
        # "CONFORMING", "CONSERVATIVE", "CONSISTENT", "CONTROLLED", "CONVENTIONAL", "CYNICAL", "DILIGENT", "DISCIPLINED",
        # "EXACTING", "FASTIDIOUS", "FINITE", "FLAWLESS", "FORMAL",  "METICULOUS", "MODEST", "NIT-PICKY",
        # "OBSERVANT", "OLD-FASHIONED", "ORDERLY", "ORGANIZED", "PAINSTAKING", "PARTICULAR", "POLISHED", "PRECISE",
        # "PRUDENT", "RIGOROUS", "SAVVY", "SENSING", "STRINGENT", "STRUCTURED", "SYSTEMATIC", "THOROUGH", "TRADITIONAL",
        # "VIGILANT", "WATCHFUL", "CAREFUL", "HISTORICAL", "PEDANTIC", "PERFECTIONIST", "RELUCTANT", "RISK-AVERSE",
        # "STRICT", "THRIFTY", "FRUGAL",

        ##Trait L Words
        # "ACCEPTING", "ACCURATE", "ACUTE", "AGILE", "AMBITIOUS", "ANALYTICAL", "ANXIOUS", "APPEALING",
        # "APPROACHABLE", "ASHAMED", "ASTUTE", "AT-EASE", "ATTRACTIVE", "AWARE", "BOLD", "CALM", "CAREFUL",
        # "CAUTIOUS", "CHATTY", "CHEERFUL", "CLEVER", "COMFORTABLE", "COMMANDING", "COMMUNICATIVE", "COMPLIANT",
        # "CONFORMING", "CONFUSED", "CONSERVATIVE", "CONSISTENT", "CONTROLLED", "CONVENTIONAL", "CONVERSATIONAL",
        # "COURAGEOUS", "CREATIVE", "DECISIVE", "DETERMINED", "DILIGENT", "DISCIPLINED", "DYNAMIC", "EFFICIENT",
        # "ENTERTAINING", "EXACTING", "EXPERIMENTAL", "EXPRESSIVE", "FACILITATING", "FARSIGHTED", "FEARLESS",
        # "FLUENT", "FORMAL", "FRUGAL", "IDEALISTIC", "IMPULSIVE", "INCISIVE", "INFLUENTIAL", "INNOVATIVE",
        # "INQUISITIVE", "INSIGHTFUL", "INTELLIGENT", "INTERACTIVE", "INVENTIVE", "JOVIAL", "KEEN", "KNOWLEDGEABLE",
        # "LAID-BACK", "LOQUACIOUS", "METICULOUS", "MODEST", "OBSERVANT", "OPPORTUNISTIC", "ORDERLY", "ORGANIZED",
        # "ORIGINAL", "OUTGOING", "PATIENT", "PERCEPTIVE", "PERFECTIONIST", "PERSISTENT", "PLACID", "POLISHED",
        # "PRECISE", "PRUDENT", "RESOURCEFUL", "RESPECTFUL", "SAVVY", "SELF-ASSURED", "SELF-CONFIDENT", "SHREWD",
        # "SOCIABLE", "SPONTANEOUS", "STEADY", "STRUCTURED", "SUPPORTIVE", "SYSTEMATIC", "TALKATIVE", "THOROUGH",
        # "VENTURESOME", "WATCHFUL",

        ##Trait I Words
        # "AGILE", "ANALYTICAL", "APPEALING", "ASTUTE", "CLEVER", "CREATIVE", "DEDUCTIVE", "EXPERIMENTAL", "FACILITATING",
        # "FARSIGHTED", "FLUENT", "FRAGILE", "INCISIVE", "INFLUENTIAL", "INNOVATIVE", "INQUISITIVE", "INSIGHTFUL",
        # "INTELLIGENT", "INVENTIVE", "KEEN", "KNOWLEDGEABLE", "ORIGINAL", "PERCEPTIVE", "PERSISTENT", "PLAYFUL",
        # "RELUCTANT", "SAVVY", "SHREWD", "SPATIAL", "LOQUACIOUS", "RESOURCEFUL", "RESPECTFUL",

        ##Trait EU Words
        # "ACCEPTING", "ACCURATE", "AGILE", "APPROACHABLE", "ATTRACTIVE", "BOLD", "CALM", "CAREFUL", "CAUTIOUS",
        # "CHEERFUL", "COMFORTABLE", "COMMUNICATIVE", "COMPLIANT", "CONFORMING", "CONSISTENT", "CONVERSATIONAL", "DARING",
        # "DEMANDING", "DETERMINED", "DISCIPLINED", "EASY-GOING", "ENDEARING", "ENTERTAINING", "EVEN-TEMPERED",
        # "EXACTING", "EXPRESSIVE", "FASHIONABLE", "FEARLESS", "FLUENT", "FORMAL", "FRUGAL", "GOOD-MIXER", "HISTORICAL",
        # "INTERACTIVE", "JOVIAL", "LAID-BACK", "LIFE-OF-THE-PARTY", "LIKABLE", "METICULOUS", "MODEST", "OPPORTUNISTIC",
        # "ORGANIZED", "OUTGOING", "PATIENT", "PERFECTIONIST", "PLACID", "PONDERING", "POPULAR", "PREDICTABLE",
        # "PURPOSEFUL", "RELAXED", "RESOLUTE", "RESTFUL", "RIGOROUS", "SAVVY", "SELF-ASSURED", "SELF-CONFIDENT",
        # "SELF-RELIANT", "SENSING", "SOCIABLE", "SPONTANEOUS", "STEADY", "STRINGENT", "STRONG-WILLED", "SYSTEMATIC",
        # "THOROUGH", "THRIFTY", "TRANQUIL", "UNHURRIED", "UNRUFFLED", "UNWAVERING", "VENTURESOME", "VIGILANT",
        # "WATCHFUL",

    ]

######Start of prediction process

    print("Dropping rows with missing values...")
    # Drop rows with missing values in dependent and independent variables for training
    train_data = RegData[indep_vars + [dep_var]].dropna()

    print("Splitting training data into features and target variable...")
    # Split training data into features (X) and target (y)
    X_train = train_data[indep_vars]
    y_train = train_data[dep_var]

    print("Performing feature selection with VarianceThreshold...")
    # Remove Low-Variance Features (Threshold = 0.01)
    selector = VarianceThreshold(threshold=0.01)
    X_train_filtered = pd.DataFrame(selector.fit_transform(X_train), columns=X_train.columns[selector.get_support()])

    # --- Print Selected Features ---
    selected_features = X_train_filtered.columns.tolist()
    print(f"Selected Features after VarianceThreshold: {selected_features}")
    # --- End Print Selected Features ---

    print("Tuning Gradient Boosting Regressor hyperparameters using GridSearchCV...")
    # Hyperparameter tuning for GradientBoostingRegressor using GridSearchCV
    param_grid = {
        'n_estimators': [100, 150, 200],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 5, 7],
        'subsample': [1.0],
        'max_features': [None, 'sqrt', 'log2']
    }
    gb_model = GradientBoostingRegressor(random_state=42)
    gb_grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')
    gb_grid_search.fit(X_train_filtered, y_train)
    best_gb_model = gb_grid_search.best_estimator_

    print(f"Best parameters for Gradient Boosting: {gb_grid_search.best_params_}")

    print("Loading test data and applying feature selection...")
    # Load the test data features and target values
    test_data = test_data[indep_vars + [dep_var]].dropna()
    X_test = test_data[indep_vars]
    y_test = test_data[dep_var]

    # Apply the same feature selection to the test data (ensure the same features are used)
    X_test_filtered = pd.DataFrame(selector.transform(X_test), columns=X_test.columns[selector.get_support()])

    print("Predicting using the best Gradient Boosting model...")
    # Predict using the best Gradient Boosting model
    y_pred_gb = best_gb_model.predict(X_test_filtered)
    gb_mse = mean_squared_error(y_test, y_pred_gb)
    print(f"Gradient Boosting Mean Squared Error: {gb_mse:.4f}")

    print("Performing cross-validation for Gradient Boosting Regressor...")
    # Cross-validation for Gradient Boosting Regressor
    gb_cv_scores = cross_val_score(best_gb_model, X_train_filtered, y_train, cv=5, scoring='neg_mean_squared_error')
    print(f"Cross-validated MSE for Gradient Boosting: {-gb_cv_scores.mean():.4f}")

    print("Calculating SHAP values for both training and test sets...")
    # SHAP Values for both Training and Test Sets
    explainer = shap.Explainer(best_gb_model, X_train_filtered)
    shap_values_train = explainer(X_train_filtered)
    shap_values_test = explainer(X_test_filtered)

    # Normalize SHAP Values (without absolute value)
    shap_weights_train = shap_values_train.values  # Raw SHAP values for train
    shap_weights_test = shap_values_test.values  # Raw SHAP values for test

    # Handle NaN or Inf values in SHAP values before normalizing
    shap_weights_train = np.nan_to_num(shap_weights_train, nan=0.0, posinf=0.0, neginf=0.0)
    shap_weights_test = np.nan_to_num(shap_weights_test, nan=0.0, posinf=0.0, neginf=0.0)

    print("Normalizing SHAP values...")
    # Normalize across all features, preserving direction
    shap_weights_train = shap_weights_train / np.max(np.abs(shap_weights_train), axis=0)
    shap_weights_test = shap_weights_test / np.max(np.abs(shap_weights_test), axis=0)

    print("Scaling features and applying SHAP weights...")
    # Scale Features and Apply SHAP Weights
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_filtered)
    X_test_scaled = scaler.transform(X_test_filtered)

    # Apply SHAP weights to the features (multiply raw SHAP values with the features)
    X_train_weighted = X_train_scaled * shap_weights_train
    X_test_weighted = X_test_scaled * shap_weights_test

    # Ensure no NaN or Inf values are in the weighted data
    X_train_weighted = np.nan_to_num(X_train_weighted)
    X_test_weighted = np.nan_to_num(X_test_weighted)

    print("Building and training Neural Network model...")
    # Build and Train Neural Network using MLPRegressor (from Scikit-learn)
    nn_model = MLPRegressor(hidden_layer_sizes=(128, 64, 32), max_iter=1000, alpha=0.01, random_state=42,
                            early_stopping=True, validation_fraction=0.1, n_iter_no_change=10)
    # Increased layers in Neural Network and regularization
    nn_model.fit(X_train_weighted, y_train)

    print("Performing cross-validation for Neural Network...")
    # Cross-validation for Neural Network Model
    nn_cv_scores = cross_val_score(nn_model, X_train_weighted, y_train, cv=5, scoring='neg_mean_squared_error')
    print(f"Cross-validated MSE for Neural Network: {-nn_cv_scores.mean():.4f}")

    print("Evaluating Neural Network model on the test set...")
    # Evaluate the model on the test set
    nn_predictions = nn_model.predict(X_test_weighted)

    # Clip predictions to ensure they lie between 0 and 10
    nn_predictions = np.clip(nn_predictions, 0, 10)

    # Calculate Mean Squared Error for Neural Network
    nn_mse = mean_squared_error(y_test, nn_predictions)
    print(f"Neural Network Mean Squared Error: {nn_mse:.4f}")

    print("Process completed.")

    # --- Save Results and Selected Features to Excel ---
    output_file = r" .xlsx" 
    #input your directory here

    # Prepare SHAP Values DataFrame
    shap_df_train = pd.DataFrame(shap_weights_train, columns=X_train_filtered.columns)
    shap_df_test = pd.DataFrame(shap_weights_test, columns=X_train_filtered.columns)

    # Prepare Neural Network Predictions DataFrame
    nn_results_df = pd.DataFrame({
        'True Values': y_test,
        'Predictions': nn_predictions
    })

    # Prepare Performance Metrics DataFrame
    performance_df = pd.DataFrame({
        'Metric': ['GBM MSE', 'NN MSE'],  # More descriptive names
        'Value': [gb_mse, nn_mse]
    })

    # Create a DataFrame for the selected features
    features_df = pd.DataFrame({'Selected Features': selected_features})

    # Append new data to Excel, without overwriting existing sheets
    with pd.ExcelWriter(output_file, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
        # shap_df_train.to_excel(writer, sheet_name=f'{dep_var}_Train_SHAP_{pd.Timestamp.now().strftime("%Y%m%d%H%M%S")}', index=False)
        # shap_df_test.to_excel(writer, sheet_name=f'{dep_var}_Test_SHAP_{pd.Timestamp.now().strftime("%Y%m%d%H%M%S")}', index=False)
        nn_results_df.to_excel(writer,
                               sheet_name=f'{dep_var}_Predictions_{pd.Timestamp.now().strftime("%Y%m%d%H%M%S")}',
                               index=False)
        performance_df.to_excel(writer,
                                sheet_name=f'{dep_var}_Performance_{pd.Timestamp.now().strftime("%Y%m%d%H%M%S")}',
                                index=False)
        features_df.to_excel(writer, sheet_name=f'{dep_var}_Selected_Features', index=False)  # Save selected features

    # --- End Save Results to Excel ---
