import pandas as pd
import numpy as np
import shap
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.neural_network import MLPRegressor
from sklearn.feature_selection import VarianceThreshold
import os
import joblib
from openpyxl import Workbook

#AS OF 02/28/2025, THIS IS THE MOST EFFECTIVE MODEL
#version update: 03/17/2025

# File paths
input_file = r"C:\Users\cessd\OneDrive\Documents\MyAmazonGuy\Project Data\Culture Ethos\02-04-2025 Exploratory Analysis - Regression.xlsx"
test_file = r"C:\Users\cessd\OneDrive\Documents\MyAmazonGuy\Project Data\Culture Ethos\02-04-2025 Exploratory Analysis - Regression.xlsx"

print("Loading training data...")
# Load the training data
RegData = pd.read_excel(input_file)

print("Loading testing data...")
# Load the testing data
test_data = pd.read_excel(test_file)

# Choose dependent variable (change as needed)
dep_var = ["A", "B", "C", "D", "EU", "L", "I"]

# Check if dependent variable exists in both training and test data
if isinstance(dep_var, list):
    dep_var = tuple(dep_var)  # Convert to tuple
if dep_var in RegData.columns and dep_var in test_data.columns:
    print(f"Dependent variable '{dep_var}' found in both datasets.")

    # Select independent variables depending on the variable that you want to predict
    indep_var = {
        #Trait A Words
        "A": ["AMBITIOUS", "ANALYTICAL", "ASTUTE", "BOLD", "CLEVER", "COMMANDING", "COURAGEOUS", "CREATIVE", "DARING",
        "DECISIVE", "DEDUCTIVE", "DEMANDING", "DETERMINED", "DOGMATIC", "DYNAMIC", "EXPERIMENTAL", "FARSIGHTED",
        "FEARLESS", "INCISIVE", "INDIVIDUALISTIC", "INFLUENTIAL", "INNOVATIVE", "INQUISITIVE", "INSIGHTFUL",
        "INTELLIGENT", "INVENTIVE", "KEEN", "KNOWLEDGEABLE", "OPPORTUNISTIC", "ORIGINAL", "PERCEPTIVE", "PERSISTENT",
        "PURPOSEFUL", "RELENTLESS", "RESOLUTE", "RESOURCEFUL", "SELF-ASSURED", "SELF-CENTERED", "SELF-CONFIDENT",
        "SELF-RELIANT", "SHREWD", "STRONG-WILLED", "THRIFTY", "UNWAVERING", "VENTURESOME", "ACUTE", "IDEALISTIC"],

        #Trait B Words
        "B": ["APPROACHABLE", "ATTRACTIVE", "CHATTY", "CHEERFUL", "COMMUNICATIVE", "CONVERSATIONAL",
        "EASILY-EXCITED", "ENDEARING", "ENTERTAINING", "EXPRESSIVE", "FACILITATING", "FASHIONABLE", "GOOD-MIXER",
        "GULLIBLE", "IMMATURE", "IMPULSIVE", "INDULGENT", "INTERACTIVE", "JOVIAL", "LIFE-OF-THE-PARTY", "LIKABLE",
        "MOODY", "OUTGOING", "PLAYFUL", "POPULAR", "SENSITIVE", "SOCIABLE",  "SPONTANEOUS",
        "STATUS-CONSCIOUS", "SUGGESTIBLE", "SUPPORTIVE", "TALKATIVE", "TEMPERAMENTAL", "APPEALING", "CRISP",
        "EMOTIONAL", "FLUENT", "LOQUACIOUS", "RESPECTFUL", "SENTIMENTAL", "SPATIAL", "RELAXED",
        "COMFORTABLE", "POLISHED", "EASY-GOING", "RESOURCEFUL","AT-EASE", "CAUTIOUS", "UNWAVERING"],

        #Trait C Words
        "C": ["ACCEPTING", "AT-EASE", "CALM", "COMFORTABLE", "EASY-GOING", "EFFICIENT", "EVEN-TEMPERED", "LAID-BACK",
        "LINGERING", "NONCHALANT", "PATIENT", "PEACEFUL", "PLACID", "PONDERING", "PREDICTABLE", "RELAXED", "RESTFUL",
        "STEADY", "TRANQUIL", "UNCONCERNED", "UNHURRIED", "UNRUFFLED", "RESTLESS", "SINGLE-MINDED", "FLUENT",
        "THOROUGH","PAINSTAKING", "PRECISE", "FASTIDIOUS", "NIT-PICKY", "SYSTEMATIC"],

        #Trait D Words
        "D": ["ANALYTICAL", "ASTUTE", "DEDUCTIVE", "FARSIGHTED", "INCISIVE", "INSIGHTFUL", "INTELLIGENT", "KEEN",
        "KNOWLEDGEABLE", "PERCEPTIVE", "SHREWD",  "ACUTE", "ACCURATE", "AWARE", "CAUTIOUS", "COMPLIANT",
        "CONFORMING", "CONSERVATIVE", "CONSISTENT", "CONTROLLED", "CONVENTIONAL", "CYNICAL", "DILIGENT", "DISCIPLINED",
        "EXACTING", "FASTIDIOUS", "FINITE", "FLAWLESS", "FORMAL",  "METICULOUS", "MODEST", "NIT-PICKY",
        "OBSERVANT", "OLD-FASHIONED", "ORDERLY", "ORGANIZED", "PAINSTAKING", "PARTICULAR", "POLISHED", "PRECISE",
        "PRUDENT", "RIGOROUS", "SAVVY", "SENSING", "STRINGENT", "STRUCTURED", "SYSTEMATIC", "THOROUGH", "TRADITIONAL",
        "VIGILANT", "WATCHFUL", "CAREFUL", "HISTORICAL", "PEDANTIC", "PERFECTIONIST", "RELUCTANT", "RISK-AVERSE",
        "STRICT", "THRIFTY", "FRUGAL"],

        #Trait L Words
        "L": ["ACCEPTING", "ACCURATE", "ACUTE", "AGILE", "AMBITIOUS", "ANALYTICAL", "ANXIOUS", "APPEALING",
        "APPROACHABLE", "ASHAMED", "ASTUTE", "AT-EASE", "ATTRACTIVE", "AWARE", "BOLD", "CALM", "CAREFUL",
        "CAUTIOUS", "CHATTY", "CHEERFUL", "CLEVER", "COMFORTABLE", "COMMANDING", "COMMUNICATIVE", "COMPLIANT",
        "CONFORMING", "CONFUSED", "CONSERVATIVE", "CONSISTENT", "CONTROLLED", "CONVENTIONAL", "CONVERSATIONAL",
        "COURAGEOUS", "CREATIVE", "DECISIVE", "DETERMINED", "DILIGENT", "DISCIPLINED", "DYNAMIC", "EFFICIENT",
        "ENTERTAINING", "EXACTING", "EXPERIMENTAL", "EXPRESSIVE", "FACILITATING", "FARSIGHTED", "FEARLESS",
        "FLUENT", "FORMAL", "FRUGAL", "IDEALISTIC", "IMPULSIVE", "INCISIVE", "INFLUENTIAL", "INNOVATIVE",
        "INQUISITIVE", "INSIGHTFUL", "INTELLIGENT", "INTERACTIVE", "INVENTIVE", "JOVIAL", "KEEN", "KNOWLEDGEABLE",
        "LAID-BACK", "LOQUACIOUS", "METICULOUS", "MODEST", "OBSERVANT", "OPPORTUNISTIC", "ORDERLY", "ORGANIZED",
        "ORIGINAL", "OUTGOING", "PATIENT", "PERCEPTIVE", "PERFECTIONIST", "PERSISTENT", "PLACID", "POLISHED",
        "PRECISE", "PRUDENT", "RESOURCEFUL", "RESPECTFUL", "SAVVY", "SELF-ASSURED", "SELF-CONFIDENT", "SHREWD",
        "SOCIABLE", "SPONTANEOUS", "STEADY", "STRUCTURED", "SUPPORTIVE", "SYSTEMATIC", "TALKATIVE", "THOROUGH",
        "VENTURESOME", "WATCHFUL"],

        #Trait I Words
        "I": ["AGILE", "ANALYTICAL", "APPEALING", "ASTUTE", "CLEVER", "CREATIVE", "DEDUCTIVE", "EXPERIMENTAL", "FACILITATING",
        "FARSIGHTED", "FLUENT", "FRAGILE", "INCISIVE", "INFLUENTIAL", "INNOVATIVE", "INQUISITIVE", "INSIGHTFUL",
        "INTELLIGENT", "INVENTIVE", "KEEN", "KNOWLEDGEABLE", "ORIGINAL", "PERCEPTIVE", "PERSISTENT", "PLAYFUL",
        "RELUCTANT", "SAVVY", "SHREWD", "SPATIAL", "RESOURCEFUL", "RESPECTFUL", "SELF-CENTERED", "PARTICULAR"],

        # last highest acc SELF-CENTERED

        # SOCIABLE
        # UNHURRIED
        # RELAXED
        # COURAGEOUS
        # LINGERING

        #Trait EU Words
        "EU": ["ACCEPTING", "ACCURATE", "AGILE", "APPROACHABLE", "ATTRACTIVE", "BOLD", "CALM", "CAREFUL", "CAUTIOUS",
        "CHEERFUL", "COMFORTABLE", "COMMUNICATIVE", "COMPLIANT", "CONFORMING", "CONSISTENT", "CONVERSATIONAL", "DARING",
        "DEMANDING", "DETERMINED", "DISCIPLINED", "EASY-GOING", "ENDEARING", "ENTERTAINING", "EVEN-TEMPERED",
        "EXACTING", "EXPRESSIVE", "FASHIONABLE", "FEARLESS", "FLUENT", "FORMAL", "FRUGAL", "GOOD-MIXER", "HISTORICAL",
        "INTERACTIVE", "JOVIAL", "LAID-BACK", "LIFE-OF-THE-PARTY", "LIKABLE", "METICULOUS", "MODEST", "OPPORTUNISTIC",
        "ORGANIZED", "OUTGOING", "PATIENT", "PERFECTIONIST", "PLACID", "PONDERING", "POPULAR", "PREDICTABLE",
        "PURPOSEFUL", "RELAXED", "RESOLUTE", "RESTFUL", "RIGOROUS", "SAVVY", "SELF-ASSURED", "SELF-CONFIDENT",
        "SELF-RELIANT", "SENSING", "SOCIABLE", "SPONTANEOUS", "STEADY", "STRINGENT", "STRONG-WILLED", "SYSTEMATIC",
        "THOROUGH", "THRIFTY", "TRANQUIL", "UNHURRIED", "UNRUFFLED", "UNWAVERING", "VENTURESOME", "VIGILANT",
        "WATCHFUL"], #FACILITATING DILIGENT
    }

    print("Dropping rows with missing values...")
    # Drop rows with missing values in dependent and independent variables for training
    indep_vars = indep_var.get(dep_var, [])  # Extract list from dictionary

    train_data = RegData[indep_vars + [dep_var]].dropna()
    test_data_filtered = test_data[indep_vars].dropna()

    # Iterate over each dependent variable
    for dep_var in dep_var:
        if dep_var not in RegData.columns or dep_var not in test_data.columns:
            print(f"Skipping {dep_var}, not found in dataset.")
            continue

        print(f"Processing {dep_var}...")

######Start of prediction process

        print("Splitting training data into features and target variable...")
        # Split training data into features (X) and target (y)
        X_train = train_data[indep_vars]
        y_train = train_data[dep_var]

        print("Performing feature selection with VarianceThreshold...")
        # Remove Low-Variance Features (Threshold = 0.01)
        selector = VarianceThreshold(threshold=0.01)
        X_train_filtered = pd.DataFrame(selector.fit_transform(X_train), columns=X_train.columns[selector.get_support()])

        # --- Print Selected Features ---
        selected_features = X_train_filtered.columns.tolist()
        print(f"Selected Features after VarianceThreshold: {selected_features}")
        # --- End Print Selected Features ---

        print("Tuning Gradient Boosting Regressor hyperparameters using GridSearchCV...")
        # Hyperparameter tuning for GradientBoostingRegressor using GridSearchCV
        param_grid = {
            'n_estimators': [100, 150, 200],
            'learning_rate': [0.01, 0.05, 0.1],
            'max_depth': [3, 5, 7],
            'subsample': [1.0],
            'max_features': [None, 'sqrt', 'log2']
        }
        gb_model = GradientBoostingRegressor(random_state=42)
        gb_grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')
        gb_grid_search.fit(X_train_filtered, y_train)
        best_gb_model = gb_grid_search.best_estimator_

        print(f"Best parameters for Gradient Boosting: {gb_grid_search.best_params_}")

        print("Loading test data and applying feature selection...")
        # Load the test data features and target values
        test_data = test_data[indep_vars + [dep_var]].dropna()
        X_test = test_data[indep_vars]
        y_test = test_data[dep_var]

        # Apply the same feature selection to the test data (ensure the same features are used)
        X_test_filtered = pd.DataFrame(selector.transform(X_test), columns=X_test.columns[selector.get_support()])

        print("Predicting using the best Gradient Boosting model...")
        # Predict using the best Gradient Boosting model
        y_pred_gb = best_gb_model.predict(X_test_filtered)
        gb_mse = mean_squared_error(y_test, y_pred_gb)
        print(f"Gradient Boosting Mean Squared Error: {gb_mse:.4f}")

        print("Performing cross-validation for Gradient Boosting Regressor...")
        # Cross-validation for Gradient Boosting Regressor
        gb_cv_scores = cross_val_score(best_gb_model, X_train_filtered, y_train, cv=5, scoring='neg_mean_squared_error')
        print(f"Cross-validated MSE for Gradient Boosting: {-gb_cv_scores.mean():.4f}")

        print("Calculating SHAP values for both training and test sets...")
        # SHAP Values for both Training and Test Sets
        explainer = shap.Explainer(best_gb_model, X_train_filtered)
        shap_values_train = explainer(X_train_filtered)
        shap_values_test = explainer(X_test_filtered)

        # Normalize SHAP Values (without absolute value)
        shap_weights_train = shap_values_train.values  # Raw SHAP values for train
        shap_weights_test = shap_values_test.values  # Raw SHAP values for test

        # Handle NaN or Inf values in SHAP values before normalizing
        shap_weights_train = np.nan_to_num(shap_weights_train, nan=0.0, posinf=0.0, neginf=0.0)
        shap_weights_test = np.nan_to_num(shap_weights_test, nan=0.0, posinf=0.0, neginf=0.0)

        print("Normalizing SHAP values...")
        # Normalize across all features, preserving direction
        shap_weights_train = shap_weights_train / np.max(np.abs(shap_weights_train), axis=0)
        shap_weights_test = shap_weights_test / np.max(np.abs(shap_weights_test), axis=0)

        print("Scaling features and applying SHAP weights...")
        # Scale Features and Apply SHAP Weights
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_filtered)
        X_test_scaled = scaler.transform(X_test_filtered)

        # Apply SHAP weights to the features (multiply raw SHAP values with the features)
        X_train_weighted = X_train_scaled * shap_weights_train
        X_test_weighted = X_test_scaled * shap_weights_test

        # Ensure no NaN or Inf values are in the weighted data
        X_train_weighted = np.nan_to_num(X_train_weighted)
        X_test_weighted = np.nan_to_num(X_test_weighted)

        print("Building and training Neural Network model...")
        # Build and Train Neural Network using MLPRegressor (from Scikit-learn)
        nn_model = MLPRegressor(hidden_layer_sizes=(128, 64, 32), max_iter=1000, alpha=0.01, random_state=42,
                                early_stopping=False, validation_fraction=0.1, n_iter_no_change=10)
        # Increased layers in Neural Network and regularization
        nn_model.fit(X_train_weighted, y_train)

        print("Performing cross-validation for Neural Network...")
        # Cross-validation for Neural Network Model
        nn_cv_scores = cross_val_score(nn_model, X_train_weighted, y_train, cv=5, scoring='neg_mean_squared_error')
        print(f"Cross-validated MSE for Neural Network: {-nn_cv_scores.mean():.4f}")

        print("Evaluating Neural Network model on the test set...")
        # Evaluate the model on the test set
        nn_predictions = nn_model.predict(X_test_weighted)

        # Clip predictions to ensure they lie between proper ranges
        if dep_var == "EU":
            nn_predictions = np.clip(nn_predictions, 8, 88)
        else:
            nn_predictions = np.clip(nn_predictions, 0, 10)
        # Calculate Mean Squared Error for Neural Network
        nn_mse = mean_squared_error(y_test, nn_predictions)
        print(f"Neural Network Mean Squared Error: {nn_mse:.4f}")

        print("Process completed.")

    ####### Only use if you want to print coefficients and biases
        # # Extract trained weights and biases
        # weights = nn_model.coefs_
        # biases = nn_model.intercepts_
        #
        # # Save weights and biases
        # joblib.dump((weights, biases), "nn_model_weights_biases.pkl")
        # print("Weights and biases saved successfully!")
        #
        # # Create a dictionary to store dataframes
        # sheets = {}
        #
        # # Convert weights into DataFrames with labeled rows & columns
        # for i, w in enumerate(weights):
        #     if i == 0:
        #         # Input layer: Label columns with first hidden layer neurons
        #         df = pd.DataFrame(w, index=indep_var, columns=[f"Neuron_{j + 1}" for j in range(w.shape[1])])
        #         sheets["Input_Layer_Weights"] = df
        #     elif i == len(weights) - 1:
        #         # Output layer: Label rows as last hidden layer neurons and column as Output Neuron
        #         df = pd.DataFrame(w, index=[f"Neuron_{j + 1}" for j in range(w.shape[0])], columns=["Output_Neuron"])
        #         sheets["Output_Layer_Weights"] = df
        #     else:
        #         # Hidden layers: Label columns with next layer neurons, rows with previous layer neurons
        #         df = pd.DataFrame(w, index=[f"Neuron_{j + 1}" for j in range(w.shape[0])],
        #                           columns=[f"Neuron_{j + 1}" for j in range(w.shape[1])])
        #         sheets[f"Hidden_Layer_{i}_Weights"] = df
        #
        # # Convert biases into DataFrames with labeled rows
        # for i, b in enumerate(biases):
        #     if i == len(biases) - 1:
        #         # Output layer bias
        #         df = pd.DataFrame(np.array(b).reshape(-1, 1), index=["Output_Neuron"], columns=["Bias"])
        #         sheets["Output_Layer_Biases"] = df
        #     else:
        #         # Hidden and input layer biases
        #         df = pd.DataFrame(np.array(b).reshape(-1, 1), index=[f"Neuron_{j + 1}" for j in range(len(b))],
        #                           columns=["Bias"])
        #         sheets[f"Hidden_Layer_{i + 1}_Biases"] = df
        #
        # # Save to an Excel file
        # with pd.ExcelWriter("TRAIT_EU_mlp_weights_biases.xlsx", engine="openpyxl") as writer:
        #     for sheet_name, df in sheets.items():
        #         df.to_excel(writer, sheet_name=sheet_name)
        #
        # print("All layer weights and biases saved to 'TRAIT_EU_mlp_weights_biases.xlsx' successfully!")
        #
        # # Print the output layer weights and biases
        # print("\nOutput Layer Weights:\n", weights[-1])
        # print("\nOutput Layer Bias:\n", biases[-1])

        # Save model
        model_filename = f"nn_model_{dep_var}.joblib"
        joblib.dump(nn_model, model_filename)
        print(f"Model saved: {model_filename}")

        # --- Save Results and Selected Features to Excel ---
        output_file = r"C:\Users\cessd\OneDrive\Documents\MyAmazonGuy\Project Data\Culture Ethos\CE Score Projection - 2025-03-17.xlsx"

        # Prepare SHAP Values DataFrame
        shap_df_train = pd.DataFrame(shap_weights_train, columns=X_train_filtered.columns)
        shap_df_test = pd.DataFrame(shap_weights_test, columns=X_train_filtered.columns)

        # Prepare Neural Network Predictions DataFrame
        nn_results_df = pd.DataFrame({
            'True Values': y_test,
            'Predictions': nn_predictions
        })

        # Prepare Performance Metrics DataFrame
        performance_df = pd.DataFrame({
            'Metric': ['GBM MSE', 'NN MSE'],  # More descriptive names
            'Value': [gb_mse, nn_mse]
        })

        # Create a DataFrame for the selected features
        features_df = pd.DataFrame({'Selected Features': selected_features})

    # Append new data to Excel, without overwriting existing sheets
    with pd.ExcelWriter(output_file, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
        # shap_df_train.to_excel(writer, sheet_name=f'{dep_var}_Train_SHAP_{pd.Timestamp.now().strftime("%Y%m%d%H%M%S")}', index=False)
        # shap_df_test.to_excel(writer, sheet_name=f'{dep_var}_Test_SHAP_{pd.Timestamp.now().strftime("%Y%m%d%H%M%S")}', index=False)
        nn_results_df.to_excel(writer,
                               sheet_name=f'{dep_var}_Predictions_{pd.Timestamp.now().strftime("%Y%m%d%H%M%S")}',
                               index=False)
        # performance_df.to_excel(writer,
        #                         sheet_name=f'{dep_var}_Performance_{pd.Timestamp.now().strftime("%Y%m%d%H%M%S")}',
        #                         index=False)
        # features_df.to_excel(writer, sheet_name=f'{dep_var}_Selected_Features', index=False)  # Save selected features

    # --- End Save Results to Excel ---
